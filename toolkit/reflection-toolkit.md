# Reflection Toolkit (Full Version)

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Toolkit

The **Reflection Toolkit** provides structured questions, prompts, and review methods to help individuals and teams:

- deepen ethical reasoning  
- examine assumptions  
- strengthen decision-making  
- document learning over time  
- integrate reflection into everyday AI practice  

Reflection is a core capability within the Framework. It ensures that AI use remains adaptive, values-led, and evidence-informed.

Use this toolkit during workshops, project reviews, curriculum design, governance meetings, or individual professional practice.

---

## 2. How to Use This Toolkit

You may use this toolkit:

- **Individually** – journaling, self-study, capability review  
- **In teams** – workshops, after-action reviews, design sessions  
- **Organisationally** – to support governance cycles and capability dashboards  

The prompts are grouped by domain so you can focus reflection on specific aspects of capability.

Each prompt may be used:

- as a stand-alone question  
- as a structured sequence  
- as part of a longer learning cycle  
  *(Orient → Assess → Apply → Reflect → Iterate)*

---

## 3. Reflection Prompts by Domain

---

### Domain 1 – AI Awareness and Orientation  
**Focus:** Understanding assumptions, data origins, and model behaviour.

#### Team Prompts

- What assumptions about knowledge or intelligence shape the AI systems we use?  
- How do we know when an output may be biased or incomplete?  
- Which data sources or perspectives appear to be missing?  
- How confident are we in explaining model limitations to colleagues or stakeholders?

#### Individual Prompts

- What shapes my understanding of AI’s strengths, limitations, and risks?  
- How aware am I of data provenance and interpretability issues?  
- Where do I need deeper conceptual grounding (e.g. bias, uncertainty, training data)?  
- How would I explain “how this system works” to a non-specialist?

**Purpose:** Strengthen epistemic literacy before designing or deploying AI-enabled processes.

---

### Domain 2 – Human–AI Co-Agency  
**Focus:** Designing meaningful human oversight.

#### Team Prompts

- Are human and AI responsibilities clear in our workflows?  
- Where do we rely on AI without examining its appropriateness?  
- Which tasks must remain human-led? Why?  
- How do we preserve human judgement and contextual expertise?

#### Individual Prompts

- What do I currently delegate to AI, and on what basis?  
- How do I ensure transparency in shared decision-making?  
- What evidence do I use to validate AI outputs?  
- How visible is my human contribution within automated processes?

**Purpose:** Reinforce clarity, accountability, and intentionality in human–AI collaboration.

---

### Domain 3 – Applied Practice and Innovation  
**Focus:** Ethical experimentation and creative problem-solving.

#### Team Prompts

- Where could AI support new forms of creativity or efficiency?  
- Are we creating safe spaces for ethical experimentation?  
- Who is included or excluded in innovation activities?  
- What assumptions shape our prototypes or experiments?

#### Individual Prompts

- What low-risk experiment could I run next?  
- How do I record and share what I learn from experimentation?  
- What does “ethical innovation” mean in my role or sector?  
- How do I balance creativity with rigour and safety?

**Purpose:** Support responsible, purposeful innovation grounded in transparency and inclusive practice.

---

### Domain 4 – Ethics, Equity and Impact  
**Focus:** Bias, fairness, inclusivity, and potential harm.

#### Team Prompts

- Who benefits, and who may be excluded or disadvantaged by our current AI use?  
- Are ethical principles embedded in workflows or treated as add-ons?  
- How do we detect, discuss, and respond to bias or drift?  
- What long-term or system-wide impacts should we anticipate?

#### Individual Prompts

- What biases or inequities might AI reinforce in my work?  
- Whose perspectives are absent from our datasets, interfaces, or decisions?  
- How do I incorporate fairness and accountability into everyday practice?  
- Which ethical or regulatory standards are most relevant to my role?

**Purpose:** Embed justice-oriented, anticipatory ethics into practical decision-making.

---

### Domain 5 – Decision-Making and Governance  
**Focus:** Explainability, documentation, oversight, and accountability.

#### Team Prompts

- Where does AI influence organisational decisions?  
- Who verifies or challenges system recommendations?  
- Are we transparent about when and how AI is used?  
- What governance mechanisms are in place to review decisions?

#### Individual Prompts

- How do I ensure that AI-influenced decisions remain explainable?  
- What documentation or logs should accompany my work?  
- Where might automation conceal complexity or human bias?  
- How can I strengthen accountability in my role?

**Purpose:** Translate governance values into clear, operational oversight practices.

---

### Domain 6 – Reflection, Learning and Renewal  
**Focus:** Continuous improvement, feedback loops, and adaptive culture.

#### Team Prompts

- What feedback loops exist, and how effective are they?  
- What have we actually changed based on past reflections?  
- How do we share learning across teams or departments?  
- What patterns appear across multiple projects or reviews?

#### Individual Prompts

- How has my understanding of AI shifted recently?  
- What assumptions have changed for me?  
- How do I incorporate reflective habits into daily work?  
- What does long-term capability growth look like for me?

**Purpose:** Strengthen organisational memory and support ongoing renewal of capability.

---

## 4. Structured Reflection Cycles

### A. Rapid Reflection (5–10 minutes)

- What happened?  
- What assumptions did we make?  
- What surprised us?  
- What needs to change next time?

*Useful for team meetings, CPD sessions, or governance reviews.*

---

### B. After-Action Review (15–30 minutes)

- What worked well?  
- What didn’t work as expected?  
- What ethical or equity issues emerged?  
- What should we repeat, stop, or redesign?  
- What will we test next?

*Ideal for post-project reviews or pilot evaluation.*

---

### C. Deep-Dive Reflection (30–60 minutes)

- What were our goals?  
- How did AI influence decisions or reasoning?  
- Where did human judgement play a critical role?  
- What were the downstream impacts (positive or negative)?  
- How were values (equity, transparency, care) enacted?  
- What do we commit to improving in the next cycle?

*Best for governance committees, leadership teams, or cross-functional groups.*

---

## 5. Reflection Log Template

| Date | Context / Activity | Key Insights | Ethical or Equity Issues | Actions & Follow-up |
|------|--------------------|--------------|--------------------------|---------------------|
|      |                    |              |                          |                     |

Use one table per reflection entry. You may store multiple logs in a portfolio.

---

## 6. 90-Day Reflection Cycle Template

**Stage 1 – Orientation**  
What domain(s) are we focusing on?  
……………………………………………………………………

**Stage 2 – Application**  
What was implemented or tested?  
……………………………………………………………………

**Stage 3 – Reflection**  
What did we learn? What changed?  
……………………………………………………………………

**Stage 4 – Renewal**  
What will we adapt or redesign next?  
……………………………………………………………………

**Stage 5 – Evidence**  
What documents, logs, or outcomes support this?  
……………………………………………………………………

---

## 7. Integrating Reflection Into Practice

Suggested strategies:

- incorporate a short reflection segment in all project, governance, or curriculum meetings  
- maintain a shared reflection folder or dashboard  
- combine this toolkit with the Self-Assessment Matrix for capability reviews  
- use reflection insights to update governance logs or AI-use statements  
- embed reflection alongside innovation, not after it  

---

## Licensing

This toolkit is released under the **Creative Commons Attribution–NonCommercial–ShareAlike 4.0 International Licence (CC BY-NC-SA 4.0)**.

Please credit:

> **Wong, J. (2026). _CloudPedagogy AI Capability Framework – Reflection Toolkit_.**

---

*End of Reflection Toolkit*  

**CloudPedagogy — AI Capability Framework (2026 Edition)**
