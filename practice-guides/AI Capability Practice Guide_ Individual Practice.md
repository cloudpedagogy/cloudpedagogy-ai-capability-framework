# **AI Capability Practice Guide: Individual Practice**

**Practical, Responsible, High-Impact AI Use for Everyday Professional Work**

## **Who This Guide Is For**

This guide is for professionals, practitioners, and knowledge workers who are already usingâ€”or are about to useâ€”AI tools in their everyday work and want to do so:

- confidently

- responsibly

- effectively

- without exposing themselves or others to unnecessary risk

You do **not** need technical expertise.  
You do **not** need to study AI theory beforehand.  
You **do** need to make sound decisions under real-world pressure.

This guide assumes you are the person responsible for the outcomeâ€”even if AI is involved.



## **Who This Guide Is *Not* For**

This guide is not designed for:

- technical model development or AI engineering

- performance tuning or optimisation tutorials

- â€œgrowth hackingâ€ or automation-at-all-costs approaches

- replacing professional judgement with AI output

If you are looking for shortcuts that remove responsibility, this guide will feel deliberately uncomfortable.

## **What You Will Be Able to Do in 30â€“60 Minutes**

By working through this guide, you will be able to:

- decide *when* AI use is appropriateâ€”and when it is not

- design a clear humanâ€“AI working relationship for a real task

- identify ethical, equity, and impact risks before harm occurs

- apply lightweight governance to your own AI use

- document decisions in a way you can justify later

- reflect on outcomes and improve your practice over time

You will also produce at least one **tangible artefact** (a documented decision, revised workflow, or reflection record) you can reuse immediately.



# **FAST START â€” USE THIS NOW**

> If you only read one section, read this one.

This Fast Start is designed to let you **use AI responsibly within 10 minutes**â€”without reading the rest of the guide first.

## **When to Use This Guide**

Use this guide when:

- you feel pressure to â€œjust use AIâ€ because others are doing so

- a task matters, but the rules are unclear

- you are not sure how much to trust an AI output

- the impact extends beyond you (colleagues, clients, students, the public)

- you might be asked later to justify how a decision was made

- you sense risk, but canâ€™t quite articulate where it lies

If none of these are true, this guide may not be necessary yet.



## **The 10-Minute Entry Workflow**

Use this sequence *before* you open an AI tool or act on its output.

### **Step 1 â€” Name the task**

Write down, in plain language:

> â€œI am using AI to help me with: **\[task\]**â€

Avoid abstract phrasing. Be concrete.

### **Step 2 â€” Decide the role of AI (Co-Agency check)**

Ask:

- What part of this task can AI support?

- What part **must remain human-led**?

If you cannot clearly answer both, stop and refine the task before proceeding.

### **Step 3 â€” Apply the primary capability check**

Ask:

- Do I understand how this AI might produce an answer?

- Do I know what it *might get wrong* here?

If not, you are not ready to rely on the output.



### **Step 4 â€” Run the rapid ethics & impact screen**

Ask:

- Who could be affected by this output besides me?

- Could this introduce bias, misrepresentation, or harm?

If impact extends beyond yourself, increase scrutiny.

### **Step 5 â€” Decide the action**

Choose **one**:

- âœ… Proceed with AI support

- ðŸ” Revise the task or prompt

- â›” Pause and escalate (seek review, expertise, or decide not to use AI)

Document the choice. Even one sentence is enough.



## **Worked Example: Same Task, Three Outcomes**

### **Task**

Using AI to draft a short professional summary for public use.

### **âœ… Good Use**

- AI produces a draft

- Human reviews for accuracy, tone, and context

- Sensitive claims are verified or removed

- Final wording is human-authored and owned

**Why this works:**
AI accelerates drafting without replacing judgement or accountability.

### **âš ï¸ Risky Use**

- AI output is lightly edited

- No check for assumptions or omissions

- Context-specific nuance is missing

**Why thisâ€™s risky:**
Errors arenâ€™t obvious, but credibility could be undermined later.



### **â›” Unacceptable Use**

- AI output is published as-is

- No awareness of audience impact

- No accountability if challenged

**Why this fails:**
Responsibility has been delegated to a system that cannot carry it.

## **Your First Artefact (Create This Now)**

Write a short **AI Use Note** (3â€“5 lines):

> *Task:  *
> *Role of AI:  *
> *Human responsibility:  *
> *Risks considered:  *
> *Decision made:*

This single note is enough to:

- clarify your own thinking

- demonstrate responsible practice

- protect you if questions arise later

You have now already improved your AI capability.



# **HOW THIS GUIDE WORKS**

This guide is designed to be **used**, not read from start to finish.

You are not expected to move linearly. You are expected to:

- dip in when a situation arises

- apply a tool

- make a judgement

- move on

This section explains **how to navigate the guide without increasing cognitive load**.

## **The Six Domains as a Living Workflow**

The AI Capability Framework is built around six domains.  
In this guide, they operate as a **practical workflow**, not a conceptual model.

You do **not** apply all six domains equally every time.  
You apply them **as the situation demands**.

### **The capability flow in practice**

- **AI Awareness & Orientation**
  Understand what the system is doing and where it might mislead.

- **Humanâ€“AI Co-Agency**
  Decide who does what, and who remains accountable.

- **Applied Practice & Innovation**
  Use AI creatively and productively, but within boundaries.

- **Ethics, Equity & Impact**
  Anticipate who may be affected and how harm could occur.

- **Decision-Making & Governance**
  Apply proportionate oversight, documentation, and review.

- **Reflection, Learning & Renewal**
  Learn from outcomes and improve your future practice.

Skipping a domain does not save time.  
It usually **moves risk downstream**, where it is harder to see and harder to correct.

## **How to Use This Guide Under Time Pressure**

Most AI decisions are made:

- mid-task

- under delivery pressure

- with incomplete information

This guide is structured to support exactly that reality.

### **If you only have 5â€“10 minutes**

- Use the **Fast Start**

- Apply the co-agency check

- Run the rapid ethics screen

- Document the decision briefly

That alone significantly improves your practice.

### **If you have 20â€“30 minutes**

- Identify which **situational entry point** fits your context

- Apply the relevant domains

- Use one template or checklist

- Capture one reflection insight

This is the *recommended minimum* for non-trivial work.

### **If stakes are high**

- Work through all domains

- Pay particular attention to ethics and governance

- Use the escalation guidance later in the guide

- Document reasoning clearly

High-impact contexts require *slower, more deliberate capability*, not faster automation.



# **SITUATIONAL ENTRY POINTS**

> Start where your problem is â€” not where the framework begins.

Most people donâ€™t begin by asking,  
â€œWhich AI capability domain should I apply?â€

They begin with a **situation**.

Use the entry point below that best matches your reality. Each points you to what to do next.

## **Entry Point 1 â€” â€œI need to move quickly, but Iâ€™m uneasy.â€**

You feel pressure to deliver fast, but something about using AI feels risky.

**Primary domains to apply**

- Humanâ€“AI Co-Agency

- Ethics, Equity & Impact

**What to do now**

- Clarify which parts of the task must remain human-led

- Identify who could be affected if the output is wrong

- Increase review before sharing or acting on the output

**Common failure mode**

- Speed displacing judgement

- Assumption that quick work is low-impact



## **Entry Point 2 â€” â€œIâ€™m not sure if AI is appropriate here.â€**

You could use AI â€” but should you?

**Primary domains to apply**

- AI Awareness & Orientation

- Decision-Making & Governance

**What to do now**

- Identify what kind of system you are interacting with

- Ask what it cannot reasonably be expected to know

- Decide whether AI should assist, inform, or be excluded

**Common failure mode**

- Treating uncertainty as a signal to â€œtry anywayâ€

## **Entry Point 3 â€” â€œThe output looks good, but I donâ€™t fully trust it.â€**

The AI result appears polished, but confidence is low.

**Primary domains to apply**

- AI Awareness & Orientation

- Reflection, Learning & Renewal



**What to do now**

- Identify assumptions or gaps in the output

- Cross-check critical claims or framings

- Adjust your future prompts or workflows

**Common failure mode**

- Mistaking fluency for reliability

## **Entry Point 4 â€” â€œThis could affect other people.â€**

The work goes beyond personal use.

**Primary domains to apply**

- Ethics, Equity & Impact

- Decision-Making & Governance

**What to do now**

- Identify impacted groups

- Consider fairness, bias, and misrepresentation

- Decide what oversight or disclosure is appropriate

**Common failure mode**

- Treating indirect impact as negligible

## **Entry Point 5 â€” â€œSomeone might question how this was produced.â€**

You anticipate scrutiny â€” now or later.

**Primary domains to apply**

- Decision-Making & Governance

- Humanâ€“AI Co-Agency

**What to do now**

- Document how AI was used and where humans intervened

- Ensure accountability is clear

- Avoid delegating judgement retroactively

**Common failure mode**

- Trying to reconstruct decisions after the fact

## **Entry Point 6 â€” â€œI want to get better, not just faster.â€**

Youâ€™re thinking about long-term capability, not one task.

**Primary domains to apply**

- Reflection, Learning & Renewal

- Applied Practice & Innovation



**What to do now**

- Identify patterns in what works and what fails

- Adjust how you design tasks and prompts

- Capture insights for future reuse

**Common failure mode**

- Repeating mistakes without noticing them

# **How the Rest of This Guide Is Structured**

From this point onward, the guide moves into the **core practice workflow**, where each domain is treated as a practical instrument.

Each domain section will:

- explain what it protects or enables

- show how to apply it immediately

- identify common failure modes

- include a short reflection moment

You can engage with:

- one domain

- several domains

- or all six

depending on the situation you face.

# **CORE PRACTICE WORKFLOW**

## **Domains 1â€“3: Working Well With AI**

These first three domains shape *how you engage with AI at the point of use*.  
They focus on understanding, role clarity, and responsible application.

## **Domain 1 â€” AI Awareness & Orientation**

### **What This Domain Protects**

This domain protects you from:

- false confidence in AI outputs

- misunderstanding what an AI system can or cannot know

- treating generated content as authoritative when it is not

AI systems do not â€œknowâ€ in human terms.  
They generate outputs based on patterns in data and probability.  
Awareness is what allows you to decide **how much trust is appropriate**.

### **Apply Now â€” Key Questions**

Before using or acting on an AI output, ask:

- What kind of system am I interacting with?

- What information *might be missing* from its training or context?

- What assumptions is the output making implicitly?

- Where would an error matter most?

If you cannot identify plausible failure points, you are over-trusting the output.

### **Tool in Use â€” Awareness Check**

Use this quick check before relying on an output:

> **AI Awareness Check**

- What is the AI optimised to do here?

- What is it *not* designed to do?

- What would a confident-sounding but wrong answer look like?

Capture one sentence for each.

### **Common Failure Modes**

- Confusing fluency with accuracy

- Assuming recent or niche knowledge without verification

- Treating the output as neutral or objective

### **Quick Reflection**

> *What did I assume the AI â€œknewâ€ that it could not reasonably know?*

Write one sentence. This improves future judgement immediately.



## **Domain 2 â€” Humanâ€“AI Co-Agency**

### **What This Domain Protects**

This domain protects:

- accountability

- professional responsibility

- ethical ownership of decisions

AI can assist, but it cannot carry responsibility.  
Co-agency makes that boundary explicit.

### **Apply Now â€” Key Questions**

Ask:

- Which parts of this task can be delegated to AI support?

- Which parts **must remain human-led**?

- Who is accountable if something goes wrong?

If accountability is unclear, co-agency is poorly designed.

### **Tool in Use â€” Co-Agency Map**

Define roles explicitly:

> **Co-Agency Map**

- AI supports by:

- Human decides on:

- Human remains accountable for:

This takes under two minutes and prevents role drift.

### **Common Failure Modes**

- Letting AI shape decisions implicitly

- Accepting suggestions without scrutiny

- â€œRubber-stampingâ€ AI outputs

### **Quick Reflection**

> *Did I actively design the relationshipâ€”or let it emerge by default?*

## **Domain 3 â€” Applied Practice & Innovation**

### **What This Domain Enables**

This domain enables:

- productive experimentation

- creative use of AI

- improvement of workflows

Innovation here means **better practice**, not maximal automation.



### **Apply Now â€” Key Questions**

Ask:

- Does AI actually improve this task?

- What would â€œgoodâ€ look like without AI?

- Am I experimenting safely, or exposing others to risk?

Not every task benefits from AI assistance.

### **Tool in Use â€” Safe Experimentation Prompt**

When exploring AI use, try:

> *â€œGenerate a draft to support ideation. I will review for accuracy, bias, and appropriateness before use.â€*

Explicitly frame AI as *support*, not authority.

### **Common Failure Modes**

- Using AI because it is available, not because it adds value

- Scaling use before understanding consequences

- Treating experimentation as risk-free

### **Quick Reflection**

> *What improved because I used AIâ€”and what didnâ€™t?*

Capture one insight for reuse later.



### **Domains 1â€“3 in Practice**

Together, these domains shape:

- how you understand AI output

- how you maintain control and responsibility

- how you innovate without carelessness

They are **necessary but not sufficient** for responsible practice.  
As impact and risk increase, you must apply the next domains.



**RISK, RESPONSIBILITY & RENEWAL**

## **Domains 4â€“6: Governing Impact and Sustaining Capability**

Domains 4â€“6 apply **when AI use affects other people, shapes decisions, or persists over time**.  
These domains turn good use into *responsible use*.

## **Domain 4 â€” Ethics, Equity & Impact**

### **What This Domain Protects**

This domain protects:

- people affected by AI-influenced decisions

- institutional trust

- social legitimacy and fairness

Ethics is not an after-the-fact check.  
It is a **design constraint**.

### **Apply Now â€” Key Questions**

Before sharing, publishing, deploying, or acting on AI output, ask:

- Who benefits from this output?

- Who might be disadvantaged, misrepresented, or excluded?

- What assumptions about people, culture, or context are embedded?

- What harm would be hardest to reverse?

If potential harm exists and is unexamined, pause.

### **Tool in Use â€” Rapid Ethical Check**

Use this 90-second scan:

> **Ethical Impact Scan**

- Accuracy risk: What might be misleading or wrong?

- Bias risk: Whose perspectives are missing or flattened?

- Power risk: Who has less ability to challenge this output?

One sentence per risk is sufficient.

### **Common Failure Modes**

- Treating ethics as compliance

- Assuming neutrality because the output â€œsounds reasonableâ€

- Overlooking who cannot contest or correct the output

### **Quick Reflection**

> *If I were on the receiving end of this output, what would concern me most?*



## **Domain 5 â€” Decision-Making & Governance**

### **What This Domain Protects**

This domain protects:

- accountability

- institutional legitimacy

- defensibility of decisions

Governance answers one question:  
**â€œHow was this decision shaped, and who takes responsibility?â€**

### **Apply Now â€” Key Questions**

Ask:

- Does AI influence this decision directly or indirectly?

- Is this a low-risk assistive use or a high-impact judgement?

- Who can challenge the outcome?

If you cannot document these answers, governance is missing.



### **Tool in Use â€” Decision Transparency Log**

For any AI-influenced judgement:

> **Decision Transparency Log**

- Decision being made:

- Role of AI (inform / suggest / generate):

- Human decision-maker:

- Review or escalation required? (Yes / No)

This log can be informalâ€”but must exist.

### **Risk Thresholds (Practical Rule)**

Escalate or require additional review when AI use involves:

- public communication

- assessment, grading, or evaluation

- clinical, legal, or safety implications

- reputational or policy consequences

### **Common Failure Modes**

- Letting AI quietly reshape decisions

- Assuming speed equals improvement

- No audit trail for future challenge

### **Quick Reflection**

> *Could I clearly explain and justify this decision six months from now?*

## **Domain 6 â€” Reflection, Learning & Renewal**

### **What This Domain Sustains**

This domain sustains:

- long-term capability

- ethical maturity

- adaptive intelligence

Without reflection, capability stagnatesâ€”even when tools improve.

### **Apply Now â€” Key Questions**

After meaningful AI use, ask:

- What worked better because of AI?

- What new risks appeared?

- What should we change next time?

Reflection turns experience into capability.



### **Tool in Use â€” Mini Reflection Cycle**

Use this short loop:

> **Reflect â†’ Adjust â†’ Reapply**

- One insight

- One improvement

- One boundary reset

This can take under five minutes.

### **Common Failure Modes**

- Repeating the same prompts uncritically

- Treating mistakes as personal failure instead of learning signals

- No collective learning across teams

### **Quick Reflection**

> *What assumption about AI shifted for me during this task?*



## **Domains 4â€“6 in Practice**

Together, these domains ensure that:

- innovation does not outrun responsibility

- decisions remain contestable and transparent

- learning compounds rather than resets

They are essential for:

- teaching

- research

- leadership

- public or organisational impact

## **The Full Capability Cycle (Operational View)**

AI capability matures through repetition of this loop:

**Awareness â†’ Co-Agency â†’ Practice â†’ Ethics â†’ Governance â†’ Reflection â†’ Renewal**

Skipping steps increases risk.  
Revisiting steps deepens capability.



# **STAGE 5 â€” CAPABILITY SELF-CHECK & WORKED EXAMPLE**

## **Making Capability Visible and Actionable (Individual Practice)**

## **Part A â€” AI Capability Self-Check (Individual)**

This is **not an assessment** and **not a scorecard**.  
Its purpose is orientation: *Where am I right now, and what should I focus on next?*

Complete this in **under five minutes**.

### **Domain 1 â€” Awareness & Orientation**

Answer honestly:

- I know how AI generates responses (at a basic level)

- I routinely check accuracy rather than trusting fluency

- I can explain AI limitations to someone else

**If mostly NO â†’** start with Domain 1 actions before increasing usage.



### **Domain 2 â€” Humanâ€“AI Co-Agency**

Consider:

- I decide *when* to use AI rather than defaulting to it

- I am clear what I delegate to AI vs what I retain

- I remain accountable for final outputs

**If unclear â†’** prioritise co-agency boundaries.

### **Domain 3 â€” Applied Practice & Innovation**

Reflect:

- I use AI to explore options, not just get answers

- I iterate rather than accept first outputs

- I treat AI as a thinking partner, not a shortcut

**If limited â†’** build safe experimentation habits.

### **Domain 4 â€” Ethics, Equity & Impact**

Ask yourself:

- I consider who may be affected by AI-influenced outputs

- I can identify potential bias or misrepresentation

- I pause when consequences are hard to reverse

**If rarely â†’** ethics must move earlier in your workflow.

### **Domain 5 â€” Decision-Making & Governance**

Check:

- I can explain how AI influenced my decisions

- I know when escalation or review is needed

- I avoid â€œinvisibleâ€ AI influence on judgement

**If missing â†’** document decisions before risk increases.

### **Domain 6 â€” Reflection & Renewal**

Finally:

- I review AI use outcomes deliberately

- I update prompts and boundaries over time

- I learn from failures, not just successes

**If inconsistent â†’** adopt a lightweight reflection habit.

### **Interpreting Your Self-Check**

- **Most gaps in Domains 1â€“2:** slow down and build foundations

- **Most gaps in Domains 4â€“5:** increase oversight before scaling use

- **Most gaps in Domain 6:** capability will stagnate without renewal

Capability grows by *rebalancing*, not maximising.

## **Part B â€” Worked Example: Individual AI Practice in Action**

This example is deliberately **ordinary**.  
No specialist role. No advanced tooling.

### **Scenario**

You must produce a **high-stakes written output** under time pressure:

- a professional report

- a critical briefing

- a proposal or analysis

You decide to use AI.

### **Domain 1 â€” Awareness in Action**

You begin by recognising:

- The model is probabilistic, not authoritative

- It may fabricate plausible-sounding details

- Fluency â‰  accuracy

You therefore:

- avoid asking for â€œfinal answersâ€

- ask for structured drafts and alternatives



### **Domain 2 â€” Co-Agency in Action**

You define roles clearly:

> **AI does:**

- suggest structure

- surface alternative framings

- help draft early versions

> **You do:**

- validate claims

- select arguments

- own conclusions

This prevents silent delegation.

### **Domain 3 â€” Applied Practice in Action**

You use AI to:

- compare multiple ways to frame the issue

- test clarity with different audiences

- generate counter-arguments

You **do not**:

- copy outputs uncritically

- bypass your own reasoning

Iteration replaces extraction.

### **Domain 4 â€” Ethics & Impact in Action**

Before sharing the output, you ask:

- Could this mislead or oversimplify?

- Are key perspectives missing?

- Who might be disadvantaged by how this is framed?

You adjust tone, emphasis, and qualifiers accordingly.

### **Domain 5 â€” Governance in Action**

You ensure:

- AI use could be declared if required

- You can explain how AI influenced the work

- High-risk claims were reviewed manually

If challenged later, the decision trail exists.

### **Domain 6 â€” Reflection in Action**

After completion, you reflect:

- What did AI genuinely improve?

- Where did it introduce risk or noise?

- What will I change next time?

You update:

- prompts

- boundaries

- decision thresholds

Capability compounds.

## **What This Example Shows**

AI capability is not about *what tool you used*.  
It is about **how you designed the interaction**.

The same logic applies across:

- teaching

- research

- leadership

- public impact

This is the **individual foundation** all other contexts build on.



# **STAGE 6 â€” QUICK-START & PERSONAL OPERATING MODEL**

## **The Individual AI Capability Playbook (At-a-Glance)**

## **Part A â€” One-Page Quick-Start (Immediate Use)**

If you only read one page of this guide, read this.

### **The Individual AI Capability Loop**

Use this loop **every time AI matters**:

**Awareness â†’ Co-Agency â†’ Practice â†’ Ethics â†’ Governance â†’ Reflection â†’ Renewal**

Skipping steps increases risk.  
Repeating steps increases capability.

### **The 60-Second Pre-Use Check**

Before using AI, ask:

1.  **Why am I using AI here?**
    (Speed, clarity, exploration â€” not avoidance.)

2.  **What will AI do â€” and what remains mine?**
    (Define the boundary.)

3.  **What would failure look like?**
    (Accuracy, harm, reputation, fairness.)

If you cannot answer these, pause.

### **The 3 Rules of Individual AI Practice**

1.  **Fluency is never proof.**
    Always verify claims that matter.

2.  **You remain accountable.**
    AI assistance does not dilute responsibility.

3.  **If impact extends beyond you, governance applies.**
    Document or escalate.

### **Common Signals to Slow Down**

- Outputs sound â€œtoo confidentâ€

- Decisions feel easier but less examined

- You could not explain the reasoning later

These are **capability warnings**, not tool failures.



## **Part B â€” Personal AI Practice Operating Model**

This is your **repeatable system**, not a checklist.

Complete this once, then revisit quarterly.

### **1ï¸âƒ£ My Valid Reasons to Use AI**

Examples:

- clarifying complex material

- exploring alternative framings

- drafting early versions

- sense-checking logic

> *I do not use AI to avoid thinking or responsibility.*

Write yours here:

### **2ï¸âƒ£ My Co-Agency Rules**

> **AI may:**
>
> **AI may not:**
>
> **I always retain responsibility for:**

This prevents unconscious delegation.

### **3ï¸âƒ£ My Ethical Red Lines**

I **pause or stop AI use** when:

- 
- 

Examples:

- risk of misrepresentation

- impact on vulnerable groups

- irreversible public consequences

### **4ï¸âƒ£ My Governance Triggers**

I **document or escalate** AI use when:

- 
- 

Examples:

- assessments or evaluations

- public communication

- policy, safety, or reputational risk



### **5ï¸âƒ£ My Reflection Habit**

After meaningful AI use, I ask:

- What helped?

- What harmed or confused?

- What will I adjust next time?

My reflection cadence:

> â˜ after every task  
> â˜ weekly  
> â˜ project-based

### **6ï¸âƒ£ My Renewal Commitments**

To keep capability current, I will:

- revisit assumptions quarterly

- update prompts and boundaries

- stay alert to new risks

Capability decays without renewal.

## **The Individual Promise**

> *I use AI deliberately, not reflexively.  
> I design my collaboration with AI.  
> I remain responsible for outcomes.  
> I reflect so my capability grows over time.*
